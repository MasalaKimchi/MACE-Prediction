# Survival Analysis Project

A compartmentalized survival analysis framework for medical imaging, inspired by the [SegFormer3D](https://github.com/OSUPCVLab/SegFormer3D) architecture structure. This project provides a clean, modular approach to survival analysis using 3D medical imaging data with state-of-the-art deep learning architectures.

## Project Structure

```
SurvivalProject/
â”œâ”€â”€ architectures/          # Model architectures and neural network implementations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ README.md          # Architecture documentation
â”‚   â””â”€â”€ resnet3d.py        # 3D ResNet implementations (ResNet18, 34, 50, 101, 152)
â”œâ”€â”€ dataloaders/           # Data loading and dataset classes
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ README.md          # Data loading documentation
â”‚   â””â”€â”€ survival_dataset.py # Survival analysis datasets with MONAI integration
â”œâ”€â”€ data/                  # Data preprocessing utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ README.md          # Data preprocessing documentation
â”‚   â””â”€â”€ preprocessing.py   # Data preprocessing functions and pipelines
â”œâ”€â”€ losses/                # Loss functions for survival analysis
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ README.md          # Loss functions documentation
â”‚   â””â”€â”€ cox_loss.py        # Cox proportional hazards loss implementation
â”œâ”€â”€ metrics/               # Evaluation metrics for survival analysis
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ README.md          # Metrics documentation
â”‚   â””â”€â”€ survival_metrics.py # C-index, Brier score, time-dependent AUC, etc.
â”œâ”€â”€ optimizers/            # Optimizer configurations and factory
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ README.md          # Optimizer documentation
â”‚   â””â”€â”€ optimizer_factory.py # Optimizer factory with multiple options
â”œâ”€â”€ augmentations/         # Data augmentation transforms
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ README.md          # Augmentation documentation
â”‚   â””â”€â”€ transforms.py      # MONAI-based image augmentation pipelines
â”œâ”€â”€ experiments/           # Experiment configurations and templates
â”‚   â””â”€â”€ survival_analysis/
â”‚       â””â”€â”€ template_experiment/
â”‚           â”œâ”€â”€ config.yaml      # YAML-based experiment configuration
â”‚           â”œâ”€â”€ run_experiment.py # Config-driven training script
â”‚           â””â”€â”€ README.md        # Experiment documentation
â”œâ”€â”€ pretrain_scripts/      # Pre-training scripts and utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ README.md          # Pre-training documentation
â”‚   â””â”€â”€ pretrain_features.py # Pre-training script for feature extraction
â”œâ”€â”€ finetune_scripts/      # Fine-tuning scripts and utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ README.md          # Fine-tuning documentation
â”‚   â”œâ”€â”€ finetune_survival.py # Main fine-tuning script (refactored)
â”‚   â”œâ”€â”€ survival_utils.py  # Core survival analysis functions
â”‚   â”œâ”€â”€ metrics_utils.py   # Evaluation metrics utilities
â”‚   â””â”€â”€ training_utils.py  # Training utilities and helpers
â”œâ”€â”€ tests/                 # Comprehensive test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ README.md          # Testing documentation
â”‚   â”œâ”€â”€ run_all_tests.py   # Test runner
â”‚   â”œâ”€â”€ test_architectures.py # Architecture tests
â”‚   â”œâ”€â”€ test_data_preprocessing.py # Preprocessing tests
â”‚   â””â”€â”€ test_dataloaders.py # Data loading tests
â”œâ”€â”€ __init__.py           # Main package imports and exports
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ LICENSE               # MIT License
â””â”€â”€ README.md            # This file
```

## Features

- **Compartmentalized Architecture**: Clean separation of concerns following [SegFormer3D](https://github.com/OSUPCVLab/SegFormer3D) structure
- **3D ResNet Models**: Support for ResNet18, ResNet34, ResNet50, ResNet101, ResNet152 with 3D convolutions
- **Survival Analysis**: Cox proportional hazards model with comprehensive metrics using [torchsurv](https://github.com/autonlab/torchsurv)
- **Medical Imaging**: [MONAI](https://monai.io/) based 3D medical image processing and augmentation
- **Config-based Training**: YAML configuration for reproducible experiments
- **Comprehensive Metrics**: C-index, Brier score, time-dependent AUC, integrated Brier score
- **Modular Design**: Easy to extend with new architectures, datasets, and loss functions
- **Refactored Codebase**: Clean, organized scripts with separated utilities for better maintainability
- **Testing Suite**: Comprehensive unit tests for all components
- **Feature Scaling**: Proper Z-score normalization with scaler persistence for radiomic features
- **Pretraining Pipeline**: Complete pretraining workflow with feature prediction and checkpoint management
- **Advanced Optimization**: Mixed precision training, AdamW optimizer, cosine annealing, gradient clipping
- **Performance Optimizations**: torch.compile support, efficient data loading, memory optimization
- **Multi-GPU Training**: Distributed Data Parallel (DDP) support with MONAI optimizations
- **Smart Caching**: MONAI SmartCacheDataset for efficient memory management across GPUs

## Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd SurvivalProject
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Activate the conda environment (if using):
```bash
conda activate torchsurv2
```

## Usage

### Config-based Training

1. Copy the template experiment:
```bash
cp -r experiments/survival_analysis/template_experiment experiments/survival_analysis/my_experiment
```

2. Edit the configuration file:
```bash
nano experiments/survival_analysis/my_experiment/config.yaml
```

3. Run the experiment:
```bash
cd experiments/survival_analysis/my_experiment
python run_experiment.py --config config.yaml
```

### Standalone Scripts

#### Pre-training with Optimizations
```bash
python pretrain_scripts/pretrain_features.py \
    --csv_path data/train.csv \
    --feature_cols feature1 feature2 feature3 \
    --resnet resnet18 \
    --batch_size 8 \
    --epochs 100 \
    --amp --compile \
    --optimizer adamw \
    --scheduler cosine \
    --max_grad_norm 1.0
```

#### Fine-tuning with Optimizations
```bash
python finetune_scripts/finetune_survival.py \
    --csv_path data/dataset.csv \
    --resnet resnet18 \
    --batch_size 6 \
    --epochs 50 \
    --lr 1e-4 \
    --amp --compile \
    --optimizer adamw \
    --scheduler cosine \
    --max_grad_norm 1.0
```

#### Fine-tuning with Pretrained Weights
```bash
python finetune_scripts/finetune_survival.py \
    --csv_path data/dataset.csv \
    --resnet resnet18 \
    --init pretrained \
    --pretrained_path pretrain_logs/pretrained_resnet.pth \
    --batch_size 6 \
    --epochs 50 \
    --amp --compile \
    --optimizer adamw \
    --scheduler cosine \
    --max_grad_norm 1.0
```

#### Multi-GPU Distributed Training (Auto-Detects GPUs)

**Quick Start:**
```bash
# Super simple - just specify your data, everything else is auto-detected!
python finetune_scripts/quick_start_multi_gpu.py \
    --csv_path data/dataset.csv

# Or with custom settings (auto-detects all available GPUs)
python finetune_scripts/launch_distributed_training.py \
    --csv_path data/dataset.csv \
    --resnet resnet50 \
    --batch_size 32 \
    --epochs 100 \
    --amp \
    --use_smart_cache

# Direct distributed script (also auto-detects GPUs)
python finetune_scripts/finetune_survival_distributed.py \
    --csv_path data/dataset.csv \
    --resnet resnet50 \
    --batch_size 32 \
    --epochs 100 \
    --amp \
    --use_smart_cache
```

**Structured Experiments (Recommended):**
```bash
# Run fine-tuning experiment with organized output
python experiments/run_finetuning_experiment.py \
    --config experiments/finetuning_experiment/config.yaml \
    --auto_name

# Results saved in: experiments/finetune_resnet18_dataset_20240101_120000/
# â”œâ”€â”€ configs/config.yaml
# â”œâ”€â”€ checkpoints/final_model.pth
# â”œâ”€â”€ logs/tensorboard/
# â””â”€â”€ results/results.json
```

### Pretraining and Feature Scaling

The project includes pretraining capabilities and a comprehensive feature scaling system for radiomic features:

#### Pretraining (Feature Prediction)

**Quick Start:**
```bash
# Super simple pretraining (auto-detects GPUs)
python pretrain_scripts/quick_start_pretraining.py \
    --csv_path data/dataset.csv \
    --feature_cols Age AgatstonScore2D MassScore

# Multi-GPU distributed pretraining
python pretrain_scripts/launch_distributed_pretraining.py \
    --csv_path data/dataset.csv \
    --feature_cols Age AgatstonScore2D MassScore \
    --resnet resnet50 \
    --batch_size 32 \
    --epochs 1000 \
    --amp \
    --use_smart_cache
```

**Structured Experiments (Recommended):**
```bash
# Run pretraining experiment with organized output
python experiments/run_pretraining_experiment.py \
    --config experiments/pretraining_experiment/config.yaml \
    --auto_name

# Results saved in: experiments/pretrain_resnet18_dataset_20240101_120000/
# â”œâ”€â”€ configs/config.yaml
# â”œâ”€â”€ checkpoints/final_model.pth
# â”œâ”€â”€ logs/tensorboard/
# â””â”€â”€ results/results.json
```

#### Feature Scaling System

#### Pretraining with Feature Scaling
```bash
python pretrain_scripts/pretrain_features.py \
    --csv_path data/train.csv \
    --feature_cols feature1 feature2 feature3 \
    --resnet resnet18 \
    --epochs 100 \
    --output pretrained_model.pth
```

#### Using Pretrained Models for Inference
```python
from data import load_pretrained_checkpoint, inverse_transform_features
from architectures import build_network

# Load checkpoint with scaler
checkpoint = load_pretrained_checkpoint('pretrained_model.pth')
model = build_network(checkpoint['resnet_type'], 1, len(checkpoint['feature_columns']))
model.load_state_dict(checkpoint['model_state_dict'])

# Predict and convert to original scale
predictions_normalized = model(image_tensor)
predictions_original = inverse_transform_features(predictions_normalized, checkpoint['feature_scaler'])
```

#### Key Benefits
- **âœ… Proper Z-score normalization**: Features are correctly scaled during training
- **âœ… Scaler persistence**: Fitted scalers are saved and can be reused
- **âœ… Inverse transformation**: Predictions can be converted back to original scale
- **âœ… Inference capability**: Models can be used for new data prediction
- **âœ… Backward compatibility**: Existing code continues to work

## Advanced Optimization Features

The project includes state-of-the-art optimization techniques for improved performance:

### Mixed Precision Training (AMP)
- **Automatic Mixed Precision**: Uses FP16 for forward pass, FP32 for loss computation
- **Memory Efficiency**: ~50% reduction in GPU memory usage
- **Speed Improvement**: 1.5-2x faster training on modern GPUs
- **Numerical Stability**: Automatic loss scaling prevents gradient underflow

### Advanced Optimizers
- **AdamW**: Decoupled weight decay for better generalization
- **Cosine Annealing**: Smooth learning rate scheduling for better convergence
- **Gradient Clipping**: Prevents exploding gradients for training stability

### Performance Optimizations
- **torch.compile**: Additional 10-20% speedup with PyTorch 2.0
- **Efficient Data Loading**: Optimized DataLoader with persistent workers
- **Memory Management**: Smart caching and batch processing

### Usage Examples

#### Basic Optimization Flags
```bash
# Enable all optimizations
--amp --compile --optimizer adamw --scheduler cosine --max_grad_norm 1.0
```

#### Configuration-based Optimization
```yaml
# config.yaml
training:
  optimizer: "adamw"
  scheduler: "cosine"
  eta_min: 1e-7
  max_grad_norm: 1.0
  amp: true
  compile: true
```

#### Expected Performance Gains
- **Training Speed**: 1.5-2x faster with mixed precision + torch.compile
- **Memory Usage**: ~50% reduction with AMP
- **Model Performance**: 2-5% improvement in C-index with better optimization
- **Training Stability**: Better convergence with gradient clipping and scheduling

## Configuration

The `config.yaml` file allows you to configure:

- **Dataset**: CSV paths, column names, image size, augmentation
- **Model**: Architecture, initialization, pretrained weights
- **Training**: Batch size, epochs, learning rate, optimizer
- **Evaluation**: Time points for metrics calculation
- **Logging**: Output paths, tensorboard, wandb integration

## Dependencies

### Core Dependencies
- **PyTorch**: Deep learning framework
- **[MONAI](https://monai.io/)**: Medical imaging AI toolkit for 3D image processing
- **[torchsurv](https://github.com/autonlab/torchsurv)**: Survival analysis library for PyTorch
- **Polars**: Fast DataFrame library for data processing
- **NumPy**: Numerical computing
- **Scikit-learn**: Machine learning utilities

### Additional Dependencies
- **PyYAML**: YAML configuration parsing
- **TensorBoard**: Experiment logging and visualization
- **PyDICOM**: DICOM medical image format support
- **Matplotlib**: Plotting and visualization
- **Pillow**: Image processing

## Detailed Folder Documentation

Each major folder contains its own README.md with detailed documentation:

- **[architectures/README.md](architectures/README.md)**: Model architectures and neural network implementations
- **[dataloaders/README.md](dataloaders/README.md)**: Data loading and dataset classes
- **[data/README.md](data/README.md)**: Data preprocessing utilities and pipelines
- **[losses/README.md](losses/README.md)**: Loss functions for survival analysis
- **[metrics/README.md](metrics/README.md)**: Evaluation metrics and scoring functions
- **[optimizers/README.md](optimizers/README.md)**: Optimizer configurations and factory
- **[augmentations/README.md](augmentations/README.md)**: Data augmentation transforms
- **[experiments/README.md](experiments/README.md)**: Experiment configurations and templates
- **[pretrain_scripts/README.md](pretrain_scripts/README.md)**: Pre-training scripts and utilities
- **[finetune_scripts/README.md](finetune_scripts/README.md)**: Fine-tuning scripts and utilities
- **[finetune_scripts/README_MULTI_GPU.md](finetune_scripts/README_MULTI_GPU.md)**: Multi-GPU training guide
- **[tests/README.md](tests/README.md)**: Testing documentation and guidelines

## Contributing

1. Follow the compartmentalized structure inspired by [SegFormer3D](https://github.com/OSUPCVLab/SegFormer3D)
2. Add new architectures to `architectures/` with corresponding tests
3. Add new datasets to `dataloaders/` with MONAI integration
4. Add new losses to `losses/` compatible with torchsurv
5. Add new metrics to `metrics/` for survival analysis
6. For training scripts, use the modular approach:
   - Add utility functions to appropriate `*_utils.py` files
   - Keep main scripts focused on high-level orchestration
   - Follow the separation between `pretrain_scripts/` and `finetune_scripts/`
7. Update `__init__.py` files to expose new functionality
8. Add comprehensive tests for new components
9. Update relevant README.md files with documentation

## Acknowledgments

This project is inspired by and follows the architectural patterns from:
- **[SegFormer3D](https://github.com/OSUPCVLab/SegFormer3D)**: Official Implementation of SegFormer3D: an Efficient Transformer for 3D Medical Image Segmentation (CVPR/W 2024)
- **[torchsurv](https://github.com/autonlab/torchsurv)**: Survival analysis library for PyTorch
- **[MONAI](https://monai.io/)**: Medical imaging AI toolkit for 3D image processing

## ðŸ§ª Experiment Management

The project includes a comprehensive experiment management system for organized results, configurations, and checkpoints:

### Structured Experiments

```bash
# List all experiments
python experiments/manage_experiments.py list

# Show experiment details
python experiments/manage_experiments.py show pretrain_resnet18_features_20240101_120000

# Compare experiments
python experiments/manage_experiments.py compare exp1 exp2

# Clean up experiments
python experiments/manage_experiments.py cleanup old_exp1 old_exp2
```

### Experiment Structure

Each experiment creates a structured directory:

```
experiments/pretrain_resnet18_dataset_20240101_120000/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml              # Complete experiment configuration
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ checkpoint_best.pth      # Best model checkpoint
â”‚   â”œâ”€â”€ checkpoint_epoch_*.pth   # Epoch checkpoints
â”‚   â””â”€â”€ final_model.pth          # Final trained model
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ tensorboard/             # TensorBoard logs
â”œâ”€â”€ results/
â”‚   â””â”€â”€ results.json             # Final results and metrics
â””â”€â”€ artifacts/                   # Additional artifacts
```

### Complete Workflow

```bash
# 1. Pretrain model
python experiments/run_pretraining_experiment.py \
    --config experiments/pretraining_experiment/config.yaml \
    --auto_name

# 2. Fine-tune model (using pretrained weights)
python experiments/run_finetuning_experiment.py \
    --config experiments/finetuning_experiment/config.yaml \
    --auto_name

# 3. Manage experiments
python experiments/manage_experiments.py list
```

## License

This project is licensed under the MIT License - see the LICENSE file for details.

